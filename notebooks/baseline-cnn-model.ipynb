{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # or wherever your src/ lives relative to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models import FashionMNISTBaseline, FashionMNISTCNN\n",
    "from src.training import train_model, evaluate\n",
    "\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts image to PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Download the training and test datasets\n",
    "full_dataset = datasets.FashionMNIST(\n",
    "    root=\"../data/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"../data/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(len(full_dataset) * 0.8)\n",
    "val_len = len(full_dataset) - train_len\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n",
      "image.squeeze() shape: torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'T-shirt/top')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = train_dataset[0]\n",
    "\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "\n",
    "print(f\"image.squeeze() shape: {image.squeeze().shape}\")\n",
    "\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Available device is: {device}\")\n",
    "base_model = FashionMNISTBaseline(\n",
    "    hidden_layers= 20, # number of units in hidden layer\n",
    "    output_dimension= len(class_names)\n",
    ")\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizers\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=base_model.parameters(), lr=0.05, momentum=0.9)\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model=base_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    epochs=20, \n",
    "    writer = writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    model=base_model,\n",
    "    data_loader=test_loader,\n",
    "    loss_func=loss_func,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights only\n",
    "save_path = Path(\"../models/fc_fashionmnist_epoch20.pth\")\n",
    "torch.save(base_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device is: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(f\"Available device is: {device}\")\n",
    "cnn_model = FashionMNISTCNN(\n",
    "    output_dimension=len(class_names)\n",
    ")\n",
    "cnn_model.to(device)\n",
    "\n",
    "# define loss function and optimizers\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(params=cnn_model.parameters(), lr=0.001)\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 0.3856, Train Accuracy: 86.26%\n",
      "Validation Loss: 0.3202, Validation Accuracy: 88.65%\n",
      "Epoch 2/20\n",
      "Train Loss: 0.2568, Train Accuracy: 90.56%\n",
      "Validation Loss: 0.2455, Validation Accuracy: 91.28%\n",
      "Epoch 3/20\n",
      "Train Loss: 0.2152, Train Accuracy: 92.12%\n",
      "Validation Loss: 0.2405, Validation Accuracy: 91.42%\n",
      "Epoch 4/20\n",
      "Train Loss: 0.1877, Train Accuracy: 93.07%\n",
      "Validation Loss: 0.2260, Validation Accuracy: 91.90%\n",
      "Epoch 5/20\n",
      "Train Loss: 0.1666, Train Accuracy: 93.87%\n",
      "Validation Loss: 0.2198, Validation Accuracy: 92.37%\n",
      "Epoch 6/20\n",
      "Train Loss: 0.1469, Train Accuracy: 94.55%\n",
      "Validation Loss: 0.2373, Validation Accuracy: 91.89%\n",
      "Epoch 7/20\n",
      "Train Loss: 0.1280, Train Accuracy: 95.25%\n",
      "Validation Loss: 0.2348, Validation Accuracy: 92.20%\n",
      "Epoch 8/20\n",
      "Train Loss: 0.1107, Train Accuracy: 95.92%\n",
      "Validation Loss: 0.2538, Validation Accuracy: 91.69%\n",
      "Epoch 9/20\n",
      "Train Loss: 0.0958, Train Accuracy: 96.41%\n",
      "Validation Loss: 0.2446, Validation Accuracy: 92.52%\n",
      "Epoch 10/20\n",
      "Train Loss: 0.0888, Train Accuracy: 96.66%\n",
      "Validation Loss: 0.2370, Validation Accuracy: 92.70%\n",
      "Epoch 11/20\n",
      "Train Loss: 0.0736, Train Accuracy: 97.25%\n",
      "Validation Loss: 0.2634, Validation Accuracy: 92.48%\n",
      "Epoch 12/20\n",
      "Train Loss: 0.0649, Train Accuracy: 97.52%\n",
      "Validation Loss: 0.2784, Validation Accuracy: 91.79%\n",
      "Epoch 13/20\n",
      "Train Loss: 0.0615, Train Accuracy: 97.75%\n",
      "Validation Loss: 0.2997, Validation Accuracy: 92.02%\n",
      "Epoch 14/20\n",
      "Train Loss: 0.0510, Train Accuracy: 98.08%\n",
      "Validation Loss: 0.3031, Validation Accuracy: 92.67%\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal_Projects/StudyPlan/fashion-mnist-classifier/notebooks/../src/training/train_loop.py:86\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, validation_loader, loss_func, optimizer, device, epochs, writer)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     val_loss, val_acc = evaluate(model, validation_loader, loss_func, device)\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m writer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal_Projects/StudyPlan/fashion-mnist-classifier/notebooks/../src/training/train_loop.py:29\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, data_loader, loss_func, optimizer, device, epoch, writer)\u001b[39m\n\u001b[32m     27\u001b[39m outputs = model(images)\n\u001b[32m     28\u001b[39m loss = loss_func(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m optimizer.step()\n\u001b[32m     32\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal_Projects/StudyPlan/fashion-mnist-classifier/fashion-mnist-classifier-venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal_Projects/StudyPlan/fashion-mnist-classifier/fashion-mnist-classifier-venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal_Projects/StudyPlan/fashion-mnist-classifier/fashion-mnist-classifier-venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=cnn_model,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    epochs=20, \n",
    "    writer=writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    model=cnn_model,\n",
    "    data_loader=test_loader,\n",
    "    loss_func=loss_func,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights only\n",
    "save_path = Path(\"../models/cnn_fashionmnist_epoch20.pth\")\n",
    "torch.save(cnn_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filters = cnn_model.conv_layers[0].block[0].weight.data.cpu().numpy()\n",
    "filters = filters - filters.min()\n",
    "filters = filters / filters.max()\n",
    "\n",
    "fig, axs = plt.subplots(8, 8, figsize=(6, 6))\n",
    "for i in range(64):\n",
    "    ax = axs[i // 8, i % 8]\n",
    "    ax.imshow(filters[i][0], cmap='gray')\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"First Conv Layer Filters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations(model, image_tensor):\n",
    "    model.eval()\n",
    "    x = image_tensor.unsqueeze(0)  # Add batch dim\n",
    "    x.to(device)\n",
    "    activations = []\n",
    "\n",
    "    # Hook into each conv block\n",
    "    for layer in model.conv_layers:\n",
    "        x = layer(x)\n",
    "        activations.append(x)\n",
    "\n",
    "    for i, act in enumerate(activations):\n",
    "        fig, axs = plt.subplots(8, 8, figsize=(12, 6))\n",
    "        for j in range(min(64, act.shape[1])):\n",
    "            ax = axs[j // 8, j % 8]\n",
    "            ax.imshow(act[0, j].detach().cpu(), cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "        plt.suptitle(f\"Activations after ConvBlock {i+1}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = test_dataset[0][0].unsqueeze(0)\n",
    "plt.imshow(x.squeeze(), cmap=\"gray\")\n",
    "plt.title(\"Original Image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to(device)\n",
    "x = cnn_model.conv_layers[0](x)\n",
    "fig, axs = plt.subplots(8, 8, figsize=(8, 8))\n",
    "\n",
    "for j in range(64):\n",
    "    ax = axs[j // 8, j % 8]\n",
    "    ax.imshow(x[0, j].detach().cpu(), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Activations after ConvBlock 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion-mnist-classifier-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
